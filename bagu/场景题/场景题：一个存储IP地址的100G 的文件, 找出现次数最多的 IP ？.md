# 场景题：一个存储IP地址的100G 的文件, 找出现次数最多的 IP ？

和大文件中存id，然后要求排序问题一样的处理思路

使用**MapReduce**的思想解决，加上**哈希分割**，先将大文件中的IP地址按照哈希函数进行分割，存到多个文件上，接着每个分片单独处理，用Hashmap统计IP出现频次，记录当前分片最大值。最后归并处理，找出所有候选IP中的最大出现次数的IP。

### 1.哈希分割（预处理阶段）

① 使用高效哈希函数计算每个IP的哈希值
② 按哈希值取模分片：hash(ip) % N → 生成N个分片文件

**分片数计算**：假设可用内存1G，每个分片限制为50MB → N=2000分片

### 2.分块统计（Map阶段）

每个分片处理时：

* 将小文件加载到内存中
* ① 使用HashMap<String, Long>统计IP出现频率
  ② 同步维护当前分片的最大值：maxIP和maxCount

### 3.全局归并（Reduce阶段）

* 读取所有中间结果文件中的最高频IP

* 在这些候选IP中找出全局出现次数最多的IP

### 4.关键问题

**1.哈希函数设计**

```java
file_index = hash(IP地址) % 256
```

这个哈希函数确保了同一个IP地址一定会被映射到同一个文件索引

**2.某个分割的文件仍然过大，怎么解决？**

若某分片的IP种类过多导致HashMap溢出，解决方案：

* 对该分片进行二次哈希分片

### 5.面试回答模板

“我会采用分布式计算中常用的分治策略：

1. **哈希分片**：将IP按哈希值分散到256个分片中，确保相同IP在同一分片；
2. **分块统计**：对每个分片使用HashMap统计频率，同时记录分片内的最大值；
3. **全局归并**：比较所有分片的最大值得到最终结果。